{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CUDA_lab3_TODO.ipynb","private_outputs":true,"provenance":[{"file_id":"1PD8axMJPIW19SKS0LZUlg-Ps3b7OlikM","timestamp":1616584547823}],"collapsed_sections":["F9PmBZql0ow4","ygwWcMU9DJmG"],"mount_file_id":"1mVnYe-KvpzyfTelVPEtj-deDiSZqxyFc","authorship_tag":"ABX9TyOQt8Yw4oaV3czjMc4qX3fI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"g4gyOZKkHDVU"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/giulianogrossi/GPUcomputing/blob/master/lab3/CUDA_lab3_TODO.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"F9PmBZql0ow4"},"source":["# CUDA setup"]},{"cell_type":"code","metadata":{"id":"p9RIwaPbVQHV"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5YlC1IOTlNb"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kv2zAXZOu02V"},"source":["%cd /home/grossi/CUDA\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iVV0CidyVeqU"},"source":["## NVCC Plugin for Jupyter notebook\n","\n","*Usage*:\n","\n","\n","*   Load Extension `%load_ext nvcc_plugin`\n","*   Mark a cell to be treated as cuda cell\n","`%%cuda --name example.cu --compile false`\n","\n","**NOTE**: The cell must contain either code or comments to be run successfully. It accepts 2 arguments. `-n | --name` - which is the name of either CUDA source or Header. The name parameter must have extension `.cu` or `.h`. Second argument -c | --compile; default value is false. The argument is a flag to specify if the cell will be compiled and run right away or not. It might be usefull if you're playing in the main function\n","\n","*  We are ready to run CUDA C/C++ code right in your Notebook. For this we need explicitly say to the interpreter, that we want to use the extension by adding `%%cu` at the beginning of each cell with CUDA code. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"X1EeyR1jBnWR"},"source":["!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7HcKDuAB-CO"},"source":["%load_ext nvcc_plugin"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5gUDpbz5TZml"},"source":["# Divergence analysis"]},{"cell_type":"code","metadata":{"id":"4GjogJPXwfaY"},"source":["#@title working directory: **divergence**\n","%cd /home/grossi/CUDA/divergence/\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VuIBtkKxcFY6"},"source":["%%writefile /home/grossi/CUDA/utils/common.h\n","#include <sys/time.h>\n","\n","#ifndef _COMMON_H\n","#define _COMMON_H\n","\n","#define CHECK(call)                                                            \\\n","{                                                                              \\\n","    const cudaError_t error = call;                                            \\\n","    if (error != cudaSuccess)                                                  \\\n","    {                                                                          \\\n","        fprintf(stderr, \"Error: %s:%d, \", __FILE__, __LINE__);                 \\\n","        fprintf(stderr, \"code: %d, reason: %s\\n\", error,                       \\\n","                cudaGetErrorString(error));                                    \\\n","    }                                                                          \\\n","}\n","\n","#define CHECK_CUBLAS(call)                                                     \\\n","{                                                                              \\\n","    cublasStatus_t err;                                                        \\\n","    if ((err = (call)) != CUBLAS_STATUS_SUCCESS)                               \\\n","    {                                                                          \\\n","        fprintf(stderr, \"Got CUBLAS error %d at %s:%d\\n\", err, __FILE__,       \\\n","                __LINE__);                                                     \\\n","        exit(1);                                                               \\\n","    }                                                                          \\\n","}\n","\n","#define CHECK_CURAND(call)                                                     \\\n","{                                                                              \\\n","    curandStatus_t err;                                                        \\\n","    if ((err = (call)) != CURAND_STATUS_SUCCESS)                               \\\n","    {                                                                          \\\n","        fprintf(stderr, \"Got CURAND error %d at %s:%d\\n\", err, __FILE__,       \\\n","                __LINE__);                                                     \\\n","        exit(1);                                                               \\\n","    }                                                                          \\\n","}\n","\n","#define CHECK_CUFFT(call)                                                      \\\n","{                                                                              \\\n","    cufftResult err;                                                           \\\n","    if ( (err = (call)) != CUFFT_SUCCESS)                                      \\\n","    {                                                                          \\\n","        fprintf(stderr, \"Got CUFFT error %d at %s:%d\\n\", err, __FILE__,        \\\n","                __LINE__);                                                     \\\n","        exit(1);                                                               \\\n","    }                                                                          \\\n","}\n","\n","#define CHECK_CUSPARSE(call)                                                   \\\n","{                                                                              \\\n","    cusparseStatus_t err;                                                      \\\n","    if ((err = (call)) != CUSPARSE_STATUS_SUCCESS)                             \\\n","    {                                                                          \\\n","        fprintf(stderr, \"Got error %d at %s:%d\\n\", err, __FILE__, __LINE__);   \\\n","        cudaError_t cuda_err = cudaGetLastError();                             \\\n","        if (cuda_err != cudaSuccess)                                           \\\n","        {                                                                      \\\n","            fprintf(stderr, \"  CUDA error \\\"%s\\\" also detected\\n\",             \\\n","                    cudaGetErrorString(cuda_err));                             \\\n","        }                                                                      \\\n","        exit(1);                                                               \\\n","    }                                                                          \\\n","}\n","\n","inline double seconds()\n","{\n","    struct timeval tp;\n","    struct timezone tzp;\n","    int i = gettimeofday(&tp, &tzp);\n","    return ((double)tp.tv_sec + (double)tp.tv_usec * 1.e-6);\n","}\n","\n","inline void device_name() {\n","    // set up device\n","    int dev = 0;\n","    cudaDeviceProp deviceProp;\n","    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","    printf(\"device %d: %s\\n\", dev, deviceProp.name);\n","    CHECK(cudaSetDevice(dev));\n","}\n","\n","typedef unsigned long ulong;\n","typedef unsigned int uint;\n","\n","#endif // _COMMON_H\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RlbVvBaXCHBs"},"source":["%%writefile /home/grossi/CUDA/divergence/div.cu\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","#include \"../utils/common.h\"\n","\n","/*\n"," * Kernel with warp divergence\n"," */\n","__global__ void evenOddDIV(int *c, const ulong N) {\n","\tulong tid = blockIdx.x * blockDim.x + threadIdx.x;\n","\tint a, b;\n","\n","\tif (!(tid % 2))   // branch divergence\n","\t\ta = 2;                  \n","\telse\n","\t\tb = 1;                  \n","\n","\t// check index\n","\tif (tid < N)\n","\t\tc[tid] = a + b;\n","}\n","\n","/*\n"," * Kernel without warp divergence\n"," */\n","__global__ void evenOddNODIV(int *c, const int N) {\n","\tint tid = blockIdx.x * blockDim.x + threadIdx.x;\n","\tint a = 0, b = 0;\n","\tunsigned int i, twoWarpSize = 2 * warpSize;\n","\n","\tint wid = tid / warpSize; \t// warp index wid = 0,1,2,3,...\n","\tif (!(wid % 2))\n","\t\ta = 2;                  // branch1: thread tid = 0-31, 64-95, ...\n","\telse\n","\t\tb = 1;                  // branch2: thread tid = 32-63, 96-127, ...\n","\n","\t// right index\n","\tif (!(wid % 2))  // even\n","\t\ti = 2 * (tid % warpSize) + (tid / twoWarpSize) * twoWarpSize;\n","\telse            // odd\n","\t\ti = 2 * (tid % warpSize) + 1 + (tid / twoWarpSize) * twoWarpSize;\n","\n","\t// check index\n","\tif (i < N) {\n","\t\tc[i] = a + b;\n","\t}\n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(int argc, char **argv) {\n","\n","\t// set up data size\n","\tint blocksize = 1024;\n","\tulong size = 1024*1024;\n","\n","\tif (argc > 1)\n","\t\tblocksize = atoi(argv[1]);\n","\tif (argc > 2)\n","\t\tsize = atoi(argv[2]);\n","\tulong nBytes = size * sizeof(int);\n","\n","\tprintf(\"Data size: %lu  -- \", size);\n","  printf(\"Data size (bytes): %lu MB\\n\", nBytes/1000000);\n","\n","\t// set up execution configuration\n","\tdim3 block(blocksize, 1);\n","\tdim3 grid((size + block.x - 1) / block.x, 1);\n","\tprintf(\"Execution conf (block %d, grid %d)\\nKernels:\\n\", block.x, grid.x);\n","\n","\t// allocate memory\n","\tint *d_C, *C;\n","\tC = (int *) malloc(nBytes);\n","\tCHECK(cudaMalloc((void** )&d_C, nBytes));\n","\n","\t// run kernel 1\n","\tdouble iStart, iElaps;\n","\tiStart = seconds();\n","\tevenOddDIV<<<grid, block>>>(d_C, size);\n","\tCHECK(cudaDeviceSynchronize());\n","\tiElaps = seconds() - iStart;\n","\tprintf(\"\\tevenOddDIV<<<%d, %d>>> elapsed time %f sec \\n\\n\", grid.x, block.x, iElaps);\n","\tCHECK(cudaGetLastError());\n","  \n","  CHECK(cudaMemcpy(C, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","\n","\t// run kernel 2\n","  CHECK(cudaMemset(d_C, 0.0, nBytes)); // reset memory\n","\tiStart = seconds();\n","\tevenOddNODIV<<<grid, block>>>(d_C, size);\n","\tiElaps = seconds() - iStart;\n","\tprintf(\"\\tevenOddNODIV<<<%d, %d>>> elapsed time %f sec \\n\\n\", grid.x, block.x, iElaps);\n","\tCHECK(cudaGetLastError());\n","\n","\tCHECK(cudaMemcpy(C, d_C, nBytes, cudaMemcpyDeviceToHost));\n","\n","\tfree(C);\n","\t// free gpu memory and reset device\n","\tCHECK(cudaFree(d_C));\n","\tCHECK(cudaDeviceReset());\n","\treturn EXIT_SUCCESS;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kMEGfjJMcX_e"},"source":["# Compilazione ed esecuzione\n","!nvcc div.cu -o div \n","!div 1024 2000000000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdQqnuH-54Ie"},"source":["# Compilazione ed esecuzione versione di debug \n","!nvcc -g -G div.cu -o div_deb\n","!div_deb 1024 2000000000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ygwWcMU9DJmG"},"source":["#Parallel Reduction"]},{"cell_type":"code","metadata":{"id":"K0bWVYl6NVz3"},"source":["#@title working directory: **reduction**\n","%cd /home/grossi/CUDA/reduction/\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WThhkz6GDMsm"},"source":["%%writefile /home/grossi/CUDA/reduction/parReduce.cu\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <assert.h>\n","\n","#include \"../utils/common.h\"\n","\n","/*\n"," *  Block by block parallel implementation with divergence (sequential schema)\n"," */\n","__global__ void blockParReduce1(int *in, int *out, ulong n) {\n","\n","\tuint tid = threadIdx.x;\n","\tulong idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// boundary check\n","\tif (idx >= n)\n","\t\treturn;\n","\n","\t// convert global data pointer to the local pointer of this block\n","\tint *thisBlock = in + blockIdx.x * blockDim.x;\n","\n","\t// in-place reduction in global memory\n","\tfor (int stride = 1; stride < blockDim.x; stride *= 2) {\n","\t\tif ((tid % (2 * stride)) == 0)\n","\t\t\tthisBlock[tid] += thisBlock[tid + stride];\n","\n","\t\t// synchronize within threadblock\n","\t\t__syncthreads();\n","\t}\n","\n","\t// write result for this block to global mem\n","\tif (tid == 0)\n","\t\tout[blockIdx.x] = thisBlock[0];\n","}\n","\n","/*\n"," *  Block by block parallel implementation without divergence (interleaved schema)\n"," */\n","__global__ void blockParReduce2(int *in, int *out, ulong n) {\n","\n","\t// TODO\n","\t\n","}\n","\n","\n","/*\n"," * MAIN: test on parallel reduction\n"," */\n","int main(void) {\n","\tint *a, *b, *d_a, *d_b;\n","\tint blockSize = 1024;            // block dim 1D\n","\tulong numBlock = 2*1024*1024;      // grid dim 1D\n","\tulong n = blockSize * numBlock;  // array dim\n","\tlong sum_CPU = 0, sum_GPU;\n","\tlong nByte = n*sizeof(int), mByte = numBlock * sizeof(int);\n","\tdouble start, stopGPU, stopCPU, speedup;\n","\n","\tprintf(\"\\n****  test on parallel reduction  ****\\n\");\n","\n","\t// init\n","\ta = (int *) malloc(nByte);\n","\tb = (int *) malloc(mByte);\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\n","\tCHECK(cudaMalloc((void **) &d_a, nByte));\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMalloc((void **) &d_b, mByte));\n","\tCHECK(cudaMemset((void *) d_b, 0, mByte));\n","\n","\t/***********************************************************/\n","\t/*                     CPU reduction                       */\n","\t/***********************************************************/\n","\tprintf(\"  Vector length: %.2f MB\\n\",n/(1024.0*1024.0));\n","\tprintf(\"\\n  CPU procedure...\\n\");\n","\tstart = seconds();\n","\tfor (ulong i = 0; i < n; i++) \n","    sum_CPU += a[i];\n","\tstopCPU = seconds() - start;\n","\tprintf(\"    Elapsed time: %f (sec) \\n\", stopCPU);\n","\tprintf(\"    sum: %lu\\n\",sum_CPU);\n","\n","\tprintf(\"\\n  GPU kernels (mem required %lu bytes)\\n\", nByte);\n","\n","\t/***********************************************************/\n","\t/*         KERNEL blockParReduce1 (divergent)              */\n","\t/***********************************************************/\n","\t// block by block parallel implementation with divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce1...\\n\");\n","\tstart = seconds();\n","\tblockParReduce1<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaGetLastError());\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\t\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\t\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++)\n","\t\tsum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\t// reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i]=1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t/***********************************************************/\n","\t/*        KERNEL blockParReduce2  (non divergent)          */\n","\t/***********************************************************/\n","\t// block by block parallel implementation without divergence\n","\tprintf(\"\\n  Launch kernel: blockParReduce2...\\n\");\n","\tstart = seconds();\n","\tblockParReduce2<<<numBlock, blockSize>>>(d_a, d_b, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstopGPU = seconds() - start;\n","\tspeedup = stopCPU/stopGPU;\n","\tprintf(\"    Elapsed time: %f (sec) - speedup %.1f\\n\", stopGPU,speedup);\n","\tCHECK(cudaGetLastError());\n","\t\n","  // memcopy D2H\n","\tCHECK(cudaMemcpy(b, d_b, mByte, cudaMemcpyDeviceToHost));\n","\t\n","  // check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++) {\n","\t\tsum_GPU += b[i];\n","  //\t\tprintf(\"b[%d] = %d\\n\",i,b[i]);\n","\t}\n","\tassert(sum_GPU == n);\n","\t\n","  // reset input vector on GPU\n","\tfor (ulong i = 0; i < n; i++) a[i] = 1;\n","\tCHECK(cudaMemcpy(d_a, a, nByte, cudaMemcpyHostToDevice));\n","\n","\t// check result\n","\tsum_GPU = 0;\n","\tfor (uint i = 0; i < numBlock; i++)\n","\t\tsum_GPU += b[i];\n","\tassert(sum_GPU == n);\n","\n","\tcudaFree(d_a);\n","\n","\tCHECK(cudaDeviceReset());\n","\treturn 0;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pIOextPZMiav"},"source":["#@title Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_60 parReduce.cu -o parReduce\n","!parReduce"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IkYKd9J32ewH"},"source":["#Istogramma di un'immagine BMP"]},{"cell_type":"code","metadata":{"id":"Rbedjc-G23MM"},"source":["#@title working directory: **histogram**\n","%cd /home/grossi/CUDA/histogram/\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G18uZY3t2kFp"},"source":["%%writefile /home/grossi/CUDA/histogram/hist.cu\n","/**\n"," * hist.cu\n"," */\n","#include <cuda_runtime.h>\n","#include <stdio.h>\n","#include <time.h>\n","#include <limits.h>\n","\n","#include \"../utils/ImageStuff.h\"\n","#include \"../utils/common.h\"\n","\n","/*\n"," * Kernel 1D that computes histogram on GPU\n"," */\n","__global__ void histogramBMP(uint *bins, const pel *imgSrc, const uint W, const uint N, const uint M) {\n","\t\n","  // TODO\n","  \n","}\n","\n","/*\n"," * Function that computes histogram on CPU\n"," */\n","void hist_CPU(uint *bins, const pel *imgSrc, const uint W, const uint H, const uint M) {\n","\tfor (int i = 0; i < W*H; i++) {\n","\t\tuint r = i / W;              // row of the source pixel\n","\t\tuint off = i - r * W;        // col of the source pixel\n","\n","\t\t//  ** byte granularity **\n","\t\tuint p = M * r + 3*off;      // src byte position of the pixel\n","\t\tpel R = imgSrc[p];\n","\t\tpel G = imgSrc[p+1];\n","\t\tpel B = imgSrc[p+2];\n","\t\tbins[R] += 1;\n","\t\tbins[G+256] += 1;\n","\t\tbins[B+512] += 1;\n","\t}\n","}\n","\n","int main(int argc, char **argv) {\n","\n","\tuint dimBlock = 1024;\n","\tpel *imgBMP_CPU;     // Where images are stored in CPU\n","\tpel *imgBMP_GPU;\t // Where images are stored in GPU\n","\n","\tuint *binsRGB_CPU, *binsRGB_GPU, *binsRGB_GPU2CPU;\n","\tuint N_bins = 3*256;\n","\tuint bin_size = N_bins*sizeof(uint);\n","\n","\tif (argc > 2)\n","\t\tdimBlock = atoi(argv[2]);\n","\telse if (argc < 2) {\n","\t\tprintf(\"\\n\\nUsage:  hist InputFilename dimBlock\\n\");\n","\t\texit(EXIT_FAILURE);\n","\t}\n","\n","\t// bins for CPU & GPU\n","\tbinsRGB_CPU = (uint*) calloc(N_bins, sizeof(uint));\n","\tbinsRGB_GPU2CPU = (uint*) malloc(bin_size);\n","\tCHECK(cudaMalloc((void**) &binsRGB_GPU, bin_size));\n","\n","\t// Create CPU memory to store the input image\n","\timgBMP_CPU = ReadBMPlin(argv[1]);\n","\tif (imgBMP_CPU == NULL) {\n","\t\tprintf(\"Cannot allocate memory for the input image...\\n\");\n","\t\texit(EXIT_FAILURE);\n","\t}\n","\n","\t// Allocate GPU buffer for image and bins\n","\tCHECK(cudaMalloc((void**) &imgBMP_GPU, IMAGESIZE));\n","\n","\t// Copy input vectors from host memory to GPU buffers.\n","\tCHECK(cudaMemcpy(imgBMP_GPU, imgBMP_CPU, IMAGESIZE, cudaMemcpyHostToDevice));\n","\n","\t// CPU histogram\n","\tdouble start = seconds();   // start time\n","\thist_CPU(binsRGB_CPU, imgBMP_CPU, WIDTH, HEIGHT, WIDTHB);\n","\tdouble stop = seconds();   // elapsed time\n","\tprintf(\"\\nCPU elapsed time %f sec \\n\\n\", stop - start);\n","\n","\t// invoke kernels (define grid and block sizes)\n","\tuint nPixels = WIDTH*HEIGHT;\n","\tint dimGrid = (nPixels + dimBlock - 1) / dimBlock;\n","\tprintf(\"\\ndimGrid = %d   dimBlock = %d\\n\",dimGrid,dimBlock);\n","\n","\tstart = seconds();   // start time\n","\thistogramBMP<<<dimGrid, dimBlock>>>(binsRGB_GPU, imgBMP_GPU, WIDTH, nPixels, WIDTHB);\n","\tCHECK(cudaDeviceSynchronize());\n","\tstop = seconds();   // elapsed time\n","\tprintf(\"\\nGPU elapsed time %f sec \\n\\n\", stop - start);\n","\n","\t// Copy output (results) from GPU buffer to host (CPU) memory.\n","\tCHECK(cudaMemcpy(binsRGB_GPU2CPU, binsRGB_GPU, bin_size, cudaMemcpyDeviceToHost));\n","\n","\tfor (int i = 0; i < N_bins/3; i++)\n","\t\tprintf(\"bin_GPU[%d] = \\t%d\\t%d\\t%d\\t -- bin_CPU[%d] = \\t%d\\t%d\\t%d\\n\", i,\n","\t\t\t\tbinsRGB_GPU2CPU[i],binsRGB_GPU2CPU[i+256],binsRGB_GPU2CPU[i+512],\n","\t\t\t\ti,binsRGB_CPU[i],binsRGB_CPU[i+256],binsRGB_CPU[i+512]);\n","\n","\t// Deallocate GPU memory\n","\tcudaFree(imgBMP_GPU);\n","\tcudaFree(binsRGB_GPU);\n","\n","\t// tracing tools spel as Parallel Nsight and Visual Profiler to show complete traces.\n","\tCHECK(cudaDeviceReset());\n","\n","\treturn (EXIT_SUCCESS);\n","}\n","\n","/*\n"," *  Read a 24-bit/pixel BMP file into a 1D linear array.\n"," *  Allocate memory to store the 1D image and return its pointer\n"," */\n","pel *ReadBMPlin(char* fn) {\n","\tstatic pel *Img;\n","\tFILE* f = fopen(fn, \"rb\");\n","\tif (f == NULL) {\n","\t\tprintf(\"\\n\\n%s NOT FOUND\\n\\n\", fn);\n","\t\texit(EXIT_FAILURE);\n","\t}\n","\n","\tpel HeaderInfo[54];\n","\tsize_t nByte = fread(HeaderInfo, sizeof(pel), 54, f); // read the 54-byte header\n","\t// extract image height and width from header\n","\tint width = *(int*) &HeaderInfo[18];\n","\timg.width = width;\n","\tint height = *(int*) &HeaderInfo[22];\n","\timg.height = height;\n","\tint RowBytes = (width * 3 + 3) & (~3);  // row is multiple of 4 pixel\n","\timg.rowByte = RowBytes;\n","\t//save header for re-use\n","\tmemcpy(img.headInfo, HeaderInfo, 54);\n","\tprintf(\"\\n Input File name: %5s  (%d x %d)   File Size=%lu\", fn, img.width, img.height, IMAGESIZE);\n","\n","\t// allocate memory to store the main image (1 Dimensional array)\n","\tImg = (pel *) malloc(IMAGESIZE);\n","\tif (Img == NULL)\n","\t\treturn Img;      // Cannot allocate memory\n","\t// read the image from disk\n","\tsize_t out = fread(Img, sizeof(pel), IMAGESIZE, f);\n","\tfclose(f);\n","\treturn Img;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FvvbqpV-3gLH"},"source":["#@title Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_60 hist.cu ../utils/ImageStuff.c -o hist\n","!hist"],"execution_count":null,"outputs":[]}]}