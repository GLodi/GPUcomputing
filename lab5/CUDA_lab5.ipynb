{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CUDA_lab5.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["F9PmBZql0ow4","7yZYCuVxpngN","iUYP4kCJhEIx","vXUIQkZLCTcG"],"mount_file_id":"1mk0QXjREp-k_J-pdFfmgoC7-EVmgPyAo","authorship_tag":"ABX9TyOFwW0SdX0Rna996siKKO9p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"g4gyOZKkHDVU"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1mk0QXjREp-k_J-pdFfmgoC7-EVmgPyAo#scrollTo=g4gyOZKkHDVU)"]},{"cell_type":"markdown","metadata":{"id":"F9PmBZql0ow4"},"source":["# ✔ CUDA setup"]},{"cell_type":"code","metadata":{"id":"p9RIwaPbVQHV"},"source":["!nvcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5YlC1IOTlNb"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iVV0CidyVeqU"},"source":["## NVCC Plugin for Jupyter notebook\n","\n","*Usage*:\n","\n","\n","*   Load Extension `%load_ext nvcc_plugin`\n","*   Mark a cell to be treated as cuda cell\n","`%%cuda --name example.cu --compile false`\n","\n","**NOTE**: The cell must contain either code or comments to be run successfully. It accepts 2 arguments. `-n | --name` - which is the name of either CUDA source or Header. The name parameter must have extension `.cu` or `.h`. Second argument -c | --compile; default value is false. The argument is a flag to specify if the cell will be compiled and run right away or not. It might be usefull if you're playing in the main function\n","\n","*  We are ready to run CUDA C/C++ code right in your Notebook. For this we need explicitly say to the interpreter, that we want to use the extension by adding `%%cu` at the beginning of each cell with CUDA code. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"X1EeyR1jBnWR"},"source":["!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7HcKDuAB-CO"},"source":["%load_ext nvcc_plugin"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kv2zAXZOu02V"},"source":["%cd /home/grossi/CUDA\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7yZYCuVxpngN"},"source":["# ✔ VS code on Colab"]},{"cell_type":"code","metadata":{"id":"DKHvaMw3puK7"},"source":["# 1. Install the colab-code package...\n","!pip install colabcode"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YIGztaKepvqO"},"source":["# 2. Import and launch...\n","from colabcode import ColabCode\n","ColabCode()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUYP4kCJhEIx"},"source":["# ✔ DeviceQuery"]},{"cell_type":"code","metadata":{"id":"kW9b_Yuxi7id"},"source":["# DeviceQuery dell'attuale device\n","!nvcc /home/grossi/CUDA/lab4/deviceQuery/deviceQuery.cu -o deviceQuery\n","!deviceQuery"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXUIQkZLCTcG"},"source":["# Moltiplicazione matriciale con SMEM (Shared Memory)\n"]},{"cell_type":"code","metadata":{"id":"lMElD9hZCUcg"},"source":["#@title working directory: **matProdSMEM**\n","%cd /home/grossi/CUDA/lab5/matProdSMEM\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Y52R0d3CA50"},"source":["%%writefile /home/grossi/CUDA/lab5/matProdSMEM/matProdSMEM.cu\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include \"../../utils/common.h\"\n","\n","#define IDX(i,j,n) (i*n+j)\n","#define ABS(x,y) (x-y>=0?x-y:y-x)\n","#define N 1024\n","#define P 1024\n","#define M 1024\n","\n","#define BLOCK_SIZE 16\n","\n","/*\n"," * Kernel for matrix product with static SMEM\n"," *      C  =  A  *  B\n"," *    (NxM) (MxP) (PxM)\n"," */\n","__global__ void matProdSMEMstatic(float* A, float* B, float* C) {\n","\t// indexes\n","\tuint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tuint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// target: compute the right sum for the given row and col\n","\tfloat sum = 0.0;\n","\n","\t// static shared memory\n","\t__shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n","\t__shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n","\n","\t// loop over blocks from block row of matrix A\n","\t// and block column of matrix B\n","\tuint numBlocks = (P + BLOCK_SIZE - 1) / BLOCK_SIZE;\n","\tfor (uint m = 0; m < numBlocks; m++) {\n","\n","\t\t// copy block from matrix to shared memory\n","\t\tuint r = m * BLOCK_SIZE + threadIdx.y;\n","\t\tuint c = m * BLOCK_SIZE + threadIdx.x;\n","\t\tAs[threadIdx.y][threadIdx.x] = A[IDX(row, c, P)];\n","\t\tBs[threadIdx.y][threadIdx.x] = B[IDX(r, col, M)];\n","\n","\t\t//---------------------------------------------------------------\n","\t\t__syncthreads();  //  BARRIER SYNC on SMEM loading\n","\n","\t\t// length of this part of row-column product is BLOCK_SIZE\n","\t\t// except for last block when it may be smaller\n","\t\tuint K = BLOCK_SIZE;\n","\t\tif (m == numBlocks - 1) K = P - m * BLOCK_SIZE; // tune last block\n","\n","\t\t// compute this part of row-column product\n","\t\tfor (uint k = 0; k < K; k++)\n","\t\t\tsum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n","\n","\t\t//---------------------------------------------------------------\n","\t\t__syncthreads();  //  BARRIER SYNC on prod over blocks\n","\t\t// Synchronize to make sure that the preceding\n","\t\t// computation is done before loading two new\n","\t\t// sub-matrices of A and B in the next iteration\n","\t}\n","\n","\t// store computed element in matrix C\n","\tif (row < N && col < M)\n","\t\tC[row * M + col] = sum;\n","}\n","\n","\n","/*\n"," * Kernel for matrix product using dynamic SMEM\n"," */\n","__global__ void matProdSMEMdynamic(float* A, float* B, float* C, const uint SMEMsize) {\n","\t// indexes\n","\tuint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tuint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// dynamic shared memory (inside or outside kernel)\n","\textern __shared__ float smem[];\n","\n","\t// Var As is manually set at beginning of shared\n","\tfloat *As = smem;\n","\t// Var Bs is manually set at the end of As\n","\tfloat *Bs = &smem[SMEMsize];\n","\n","\t// loop over blocks from block row of matrix A\n","\t// and block column of matrix B\n","\tfloat sum = 0.0;\n","\tuint numBlocks = (P + blockDim.x - 1) / blockDim.x;\n","\tfor (uint m = 0; m < numBlocks; m++) {\n","\n","\t\t// copy block from matrix to shared memory\n","\t\tuint c = m * blockDim.x + threadIdx.x;\n","\t\tuint r = m * blockDim.y + threadIdx.y;\n","\t\tAs[threadIdx.y * blockDim.y + threadIdx.x] = A[IDX(row, c, P)];\n","\t\tBs[threadIdx.y * blockDim.y + threadIdx.x] = B[IDX(r, col, M)];\n","\n","\t\t//---------------------------------------------------------------\n","\t\t__syncthreads();\n","\n","\t\t// length of this part of row-column product is BLOCK_SIZE\n","\t\t// except for last block when it may be smaller\n","\t\tuint K = (m == numBlocks - 1 ? P - m * blockDim.x : blockDim.x);\n","\n","\t\t// compute this part of row-column product\n","\t\tfor (int k = 0; k < K; k++)\n","\t\t\tsum += As[threadIdx.y * blockDim.x + k] * Bs[k * blockDim.y + threadIdx.x];\n","\n","\t\t//---------------------------------------------------------------\n","\t\t__syncthreads();\n","\t}\n","\n","\t// store computed element in matrix C\n","\tif (row < N && col < M)\n","\t\tC[row * M + col] = sum;\n","}\n","\n","/*\n"," * Kernel for naive matrix product\n"," */\n","__global__ void matProd(float* A, float* B, float* C) {\n","\t// indexes\n","\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","\t// each thread computes an entry of the product matrix\n","\tif ((row < N) && (col < M)) {\n","\t\tfloat sum = 0;\n","\t\tfor (int k = 0; k < P; k++)\n","\t\t\tsum += A[row * P + k] * B[k * M + col];\n","\t\tC[row * M + col] = sum;\n","\t}\n","}\n","\n","/*\n"," *  matrix product on CPU\n"," */\n","void matProdCPU(float* A, float* B, float* C) {\n","\n","\tfor (int i = 0; i < N; i++)\n","\t\tfor (int j = 0; j < M; j++) {\n","\t\t\tfloat sum = 0;\n","\t\t\tfor (int k = 0; k < P; k++)\n","\t\t\t\tsum += A[i * P + k] * B[k * M + j];\n","\t\t\tC[i * M + j] = sum;\n","\t\t}\n","}\n","\n","/*\n"," * Test the device\n"," */\n","unsigned long testCUDADevice(void) {\n","\tint dev = 0;\n","\n","\tcudaDeviceSetCacheConfig (cudaFuncCachePreferEqual);\n","\tcudaDeviceProp deviceProp;\n","\tcudaSetDevice(dev);\n","\tcudaGetDeviceProperties(&deviceProp, dev);\n","\tprintf(\"Device %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n","\tprintf(\"Total amount of shared memory available per block: %lu KB\\n\",\n","\t\t\tdeviceProp.sharedMemPerBlock / 1024);\n","\treturn deviceProp.sharedMemPerBlock;\n","}\n","\n","\n","/*\n"," * elementwise comparison between two mqdb\n"," */\n","void checkResult(float *A, float *B) {\n","\tdouble epsilon = 1.0E-8;\n","\tbool match = 1;\n","\tfor (int i = 0; i < N*M; i++)\n","\t\tif (ABS(A[i], B[i]) > epsilon) {\n","\t\t\tmatch = 0;\n","\t\t\tprintf(\"   * Arrays do not match!\\n\");\n","\t\t\tbreak;\n","\t\t}\n","\tif (match)\n","\t\tprintf(\"   Arrays match\\n\\n\");\n","}\n","\n","/*\n"," * MAIN\n"," */\n","int main(void) {\n","\t // Kernels for matrix product\n","\t //      C  =  A  *  B\n","\t //    (NxM) (MxP) (PxM)\n","\tuint rowA = N, rowB = P;\n","\tuint colA = P, colB = M;\n","\tuint rowC = N, colC = M;\n","\tfloat *A, *B, *C, *C1;\n","\tfloat *dev_A, *dev_B, *dev_C;\n","\n","\t// dims\n","\tunsigned long Asize = rowA * colA * sizeof(float);\n","\tunsigned long Bsize = rowB * colB * sizeof(float);\n","\tunsigned long Csize = rowC * colC * sizeof(float);\n","\tunsigned long maxSMEMbytes;\n","\tuint nByteSMEM = 2 * BLOCK_SIZE * BLOCK_SIZE * sizeof(float);\n","\tprintf(\"N = %d, M = %d, P = %d\\n\",N,M,P);\n","\n","\t// test device shared memory\n","\tmaxSMEMbytes = testCUDADevice();\n","\tif (maxSMEMbytes < nByteSMEM)\n","\t\tprintf(\"Shared memory usage WARNING: available: %lu, required: %d bytes\\n\",\n","\t\t\t\tmaxSMEMbytes, nByteSMEM);\n","\telse\n","\t\tprintf(\"Total amount of shared memory required per block %.1f KB\\n\",\n","\t\t\t\t(float) nByteSMEM / (float) 1024);\n","\n","\t// malloc host memory\n","\tA = (float*) malloc(Asize);\n","\tB = (float*) malloc(Bsize);\n","\tC = (float*) malloc(Csize);\n","\tC1 = (float*) malloc(Csize);\n","\n","\t// malloc device memory\n","\tCHECK(cudaMalloc((void** )&dev_A, Asize));\n","\tCHECK(cudaMalloc((void** )&dev_B, Bsize));\n","\tCHECK(cudaMalloc((void** )&dev_C, Csize));\n","\tprintf(\"Total amount of allocated memory on GPU %lu bytes\\n\\n\",\n","\t\t\tAsize + Bsize + Csize);\n","\n","\t// fill the matrices A and B\n","\tfor (int i = 0; i < N * P; i++)\n","\t\tA[i] = rand() % 10;\n","\tfor (int i = 0; i < P * M; i++)\n","\t\tB[i] = rand() % 10;\n","\tmatProdCPU(A, B, C);\n","\n","\t// copy matrices A and B to the GPU\n","\tCHECK(cudaMemcpy(dev_A, A, Asize, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemcpy(dev_B, B, Bsize, cudaMemcpyHostToDevice));\n","\n","\t/***********************************************************/\n","\t/*              GPU matProdSMEM static SMEM               */\n","\t/***********************************************************/\n","\t// grid block dims = shared mem dims = BLOCK_SIZE\n","\tdim3 block(BLOCK_SIZE, BLOCK_SIZE);\n","\tdim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n","\tdouble start = seconds();\n","\tmatProdSMEMstatic<<<grid, block>>>(dev_A, dev_B, dev_C);\n","\tCHECK(cudaDeviceSynchronize());\n","\tprintf(\"   Kernel matProdSMEM static elapsed time GPU = %f\\n\", seconds() - start);\n","\n","\t// copy the array 'C' back from the GPU to the CPU\n","\tCHECK(cudaMemcpy(C1, dev_C, Csize, cudaMemcpyDeviceToHost));\n","\tcheckResult(C,C1);\n","\n","\t/***********************************************************/\n","\t/*            GPU matProdSMEMD dynamic SMEM                */\n","\t/***********************************************************/\n","\t// set cache size\n","\tcudaDeviceSetCacheConfig (cudaFuncCachePreferShared);\n","\n","\t// try with various SMEM sizes\n","\tuint sizes[] = {8,16,32};\n","\tfor (int i = 0; i < 3; i++) {\n","\t\tuint blockSize = sizes[i];\n","\t\tblock.x = blockSize;\n","\t\tblock.y = blockSize;\n","\t\tgrid.x = (M + block.x - 1) / block.x;\n","\t\tgrid.y = (N + block.y - 1) / block.y;\n","\t\tuint SMEMsize = blockSize * blockSize;\n","\t\tuint SMEMbyte = 2 * SMEMsize * sizeof(float);\n","\t\tstart = seconds();\n","\t\tmatProdSMEMdynamic<<< grid, block, SMEMbyte >>>(dev_A, dev_B, dev_C, SMEMsize);\n","\t\tCHECK(cudaDeviceSynchronize());\n","\t\tprintf(\"   Kernel matProdSMEM dynamic (SMEM size %d) elapsed time GPU = %f\\n\", blockSize, seconds() - start);\n","\n","\t\t// copy the array 'C' back from the GPU to the CPU\n","\t\tCHECK(cudaMemcpy(C1, dev_C, Csize, cudaMemcpyDeviceToHost));\n","\t\tcheckResult(C,C1);\n","\t}\n","\n","\t// free the memory allocated on the GPU\n","\tcudaFree(dev_A);\n","\tcudaFree(dev_B);\n","\tcudaFree(dev_C);\n","\n","\tcudaDeviceReset();\n","\treturn EXIT_SUCCESS;\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0PSc9B9PDTWt"},"source":["# Compilazione ed esecuzione\n","\n","!nvcc -arch=sm_60 matProdSMEM.cu  -o matProdSMEM\n","!matProdSMEM"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfR471rze2p-"},"source":["!ls -la"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOFMQZAkjlLW"},"source":["# Convoluzione con SMEM"]},{"cell_type":"code","metadata":{"id":"-_tGfHL2aRiB"},"source":["#@title working directory: **conv**\n","%cd /home/grossi/CUDA/lab5/conv/\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v9nRkLgeB10A"},"source":["%%writefile /home/grossi/CUDA/lab5/conv/conv1D.cu\n","\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include \"../../utils/common.h\"\n","\n","#define MASK_RADIUS  5\n","#define MASK_SIZE    2 * MASK_RADIUS + 1\n","#define BLOCK_SIZE   128\n","#define TILE_WIDTH   BLOCK_SIZE + MASK_SIZE - 1\n","\n","__device__ __constant__ float d_mask[MASK_SIZE];\n","\n","void initialData(float*, int);\n","void movingAverage(float*, int n);\n","void printData(float*, const int);\n","void convolutionHost(float*, float*, float*, const int);\n","void checkResult(float*, float*, int);\n","\n","/*\n"," * kernel for 1D convolution: it holds only if MASK_RADIUS < BLOCK_SIZE\n"," */\n","__global__ void convolution1D(float *result, float *data, int n) {\n","\tunsigned int i = blockDim.x * blockIdx.x + threadIdx.x;\n","\n","\t// shared memory size = BLOCK_SIZE + MASK\n","\t__shared__ float tile[TILE_WIDTH];\n","\n","\t// boundary\n","\tint left = blockIdx.x * blockDim.x - MASK_RADIUS;\n","\tint right = (blockIdx.x + 1) * blockDim.x;\n","\n","  // left halo\n","\tif (threadIdx.x < MASK_RADIUS)                      \n","\t\ttile[threadIdx.x] = left < 0 ? 0 : data[left + threadIdx.x];\n","\n","  // center\n","\ttile[threadIdx.x + MASK_RADIUS] = data[i];\n","\n","  // right halo  \n","\tif (threadIdx.x >= blockDim.x - MASK_RADIUS)  \n","\t\ttile[threadIdx.x + MASK_SIZE - 1] = right >= n ? 0 :\n","\t\t\t\tdata[right + threadIdx.x - blockDim.x + MASK_RADIUS];\n","\n","\t__syncthreads();\n","\n","\t// convolution: tile * mask\n","\tfloat sum = 0;\n","\tfor (int i = -MASK_RADIUS; i <= MASK_RADIUS; i++)\n","\t\tsum += tile[threadIdx.x + MASK_RADIUS + i] * d_mask[i + MASK_RADIUS];\n","\n","\t// final result\n","\tresult[i] = sum;\n","}\n","\n","/*\n"," * MAIN: convolution 1D host & device\n"," */\n","int main(int argc, char **argv) {\n","\t// set up device\n","\tint dev = 0;\n","\tcudaDeviceProp deviceProp;\n","\tCHECK(cudaGetDeviceProperties(&deviceProp, dev));\n","\tprintf(\"starting conv1D at device %d: %s\\n\", dev, deviceProp.name);\n","\tCHECK(cudaSetDevice(dev));\n","\n","\t// set up array size\n","\tint n = 1 << 24;\n","\tint N = MASK_SIZE;\n","\n","\tprintf(\"Array of size = %.1f MB\\n\", n/(1024.0*1024.0));\n","\n","\t// mem sizes\n","\tsize_t nBytes = n * sizeof(float);\n","\tsize_t nBytes_mask = N * sizeof(float);\n","\n","\t// grid configuration\n","\tdim3 block(BLOCK_SIZE);\n","\tdim3 grid((n + BLOCK_SIZE - 1) / BLOCK_SIZE);\n","\n","\t// allocate host memory\n","\tfloat *h_data = (float *) malloc(nBytes);\n","\tfloat *h_result = (float *) malloc(nBytes);\n","\tfloat *result = (float *) malloc(nBytes);\n","\tfloat *h_mask = (float *) malloc(nBytes_mask);\n","\n","\t//  initialize host array\n","\tmovingAverage(h_mask, N);\n","\tinitialData(h_data, n);\n","\n","\t// convolution on host\n","\tdouble start = seconds();\n","\tconvolutionHost(h_data, result, h_mask, n);\n","\tdouble hostElaps = seconds() - start;\n","\n","\t// allocate device memory\n","\tfloat *d_data, *d_result;\n","\tCHECK(cudaMalloc((void**)&d_data, nBytes));\n","\tCHECK(cudaMalloc((void**)&d_result, nBytes));\n","\n","\t// copy data from host to device\n","\tCHECK(cudaMemcpy(d_data, h_data, nBytes, cudaMemcpyHostToDevice));\n","\tCHECK(cudaMemcpyToSymbol(d_mask, h_mask, nBytes_mask));\n","\n","\tstart = seconds();\n","\tconvolution1D<<<grid, block>>>(d_result, d_data, n);\n","\tCHECK(cudaDeviceSynchronize());\n","\tdouble devElaps = seconds() - start;\n","  printf(\"Times:\\n\");\n","\tprintf(\"   - CPU elapsed time = %f\\n\", hostElaps);\n","  printf(\"   - GPU elapsed time = %f\\n\", devElaps);\n","  printf(\"   - Speed-up (ratio) = %f\\n\", hostElaps / devElaps);\n","\n","\tCHECK(cudaMemcpy(h_result, d_result, nBytes, cudaMemcpyDeviceToHost));\n","\n","\t// check result\n","\tcheckResult(h_result, result, n);\n","\n","\t// free host and device memory\n","\tCHECK(cudaFree(d_result));\n","\tCHECK(cudaFree(d_data));\n","\tfree(h_data);\n","\tfree(h_mask);\n","\tfree(h_result);\n","\tfree(result);\n","\n","\t// reset device\n","\tCHECK(cudaDeviceReset());\n","\treturn EXIT_SUCCESS;\n","}\n","\n","void initialData(float *h_data, int n) {\n","\t// initialize the data\n","\tfor (int i = 0; i < n; i++)\n","\t\th_data[i] = 10.0;\n","}\n","\n","void movingAverage(float *h_mask, int n) {\n","\t// initialize mask moving average\n","\tfor (int i = 0; i < n; i++)\n","\t\th_mask[i] = 1.0 / ((float) n);\n","\treturn;\n","}\n","\n","void printData(float *a, const int size) {\n","\tprintf(\"\\n\");\n","\tfor (int i = 0; i < size; i++)\n","\t\tprintf(\"%.2f \", a[i]);\n","\tprintf(\"\\n\");\n","\treturn;\n","}\n","\n","void convolutionHost(float *data, float *result, float *mask, const int n) {\n","\tfor (int i = 0; i < n; i++) {\n","\t\tfloat sum = 0;\n","\t\tfor (int j = 0; j < MASK_SIZE; j++) {\n","\t\t\tint idx = i - MASK_RADIUS + j;\n","\t\t\tif (idx >= 0 && idx < n)\n","\t\t\t\tsum += data[idx] * mask[j];\n","\t\t}\n","\t\tresult[i] = sum;\n","\t}\n","}\n","\n","void checkResult(float *d_result, float *h_result, int n) {\n","\tdouble epsilon = 1.0E-8;\n","\n","\tfor (int i = 0; i < n; i++)\n","\t\tif (abs(h_result[i] - d_result[i]) > epsilon) {\n","\t\t\tprintf(\"different on entry (%d) |h_result - d_result| >  %f\\n\", i,\n","\t\t\t\t\tepsilon);\n","\t\t\tbreak;\n","\t\t}\n","}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLxZjCx8bT3s"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_60  conv1D.cu -o conv1D\n","!./conv1D"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bwcTDn6ehJr_"},"source":["%%writefile /home/grossi/CUDA/lab5/conv/conv2D.cu\n","#include <stdlib.h>\n","#include <stdio.h>\n","#include <string.h>\n","\n","#include \"../../utils/common.h\"\n","\n","#define DATA_WIDTH   (24)\n","#define DATA_HEIGHT  (24)\n","#define BLOCK_SIZE   8\n","#define MASK_RADIUS  2\n","#define MASK_SIZE    (2 * MASK_RADIUS + 1)\n","#define TILE_WIDTH   (BLOCK_SIZE + MASK_SIZE - 1)\n","#define DEBUG 1\n","\n","// constant mem\n","__constant__ float M_dev[MASK_SIZE*MASK_SIZE];\n","\n","/*\n"," * kernel for convolution 2D (it holds only if MASK_RADIUS < BLOCK_SIZE)\n"," */\n","__global__ void conv2D(float *A, float *B) {\n","\tint x = blockIdx.x * blockDim.x + threadIdx.x;\n","\tint y = blockIdx.y * blockDim.y + threadIdx.y;\n","\tint RAD = MASK_RADIUS;\n","  int BmR = BLOCK_SIZE - RAD;\n","  int W = DATA_WIDTH;\n","  int H = DATA_HEIGHT;\n","\tint m = MASK_SIZE;\n","\n","\t// shared mem\n","\t__shared__ float A_s[TILE_WIDTH][TILE_WIDTH];\n","\n","  // START SHARED MEMORY LOADING\n","\n","  // 1. copy the tile upper halo \n","  if ((threadIdx.y < RAD) ) {\n","    \n","    // left corner\n","    if (threadIdx.x < RAD && (x-RAD) >= 0 && (y-RAD) >= 0)\n","      A_s[threadIdx.y][threadIdx.x] = A[(y-RAD) * W + x - RAD];\n","\n","    // right corner\n","    if (threadIdx.x >= BmR && (x+RAD) < W && (y-RAD) >= 0) \n","      A_s[threadIdx.y][threadIdx.x + 2*RAD] = A[(y-RAD) * W + x + RAD];\n","    \n","    // edge\n","    if ((y-RAD) >= 0) \n","      A_s[threadIdx.y][threadIdx.x + RAD] = A[(y-RAD) * W + x ];  \n","  }\n","\n","  // 2. copy the tile bottom halo \n","  if (threadIdx.y >= BmR) {\n","    \n","    // left corner\n","    if (threadIdx.x < RAD && (x-RAD) >= 0 && (y+RAD) < H)\n","      A_s[threadIdx.y + 2*RAD][threadIdx.x] = A[(y+RAD) * W + x - RAD];\n","\n","    // right corner\n","    if (threadIdx.x >= BmR && (y+RAD) < H) \n","      A_s[threadIdx.y + 2*RAD][threadIdx.x + 2*RAD] = A[(y+RAD) * W + x + RAD];\n","    \n","    // edge\n","    if ((y+RAD) < H) \n","      A_s[threadIdx.y + 2*RAD][threadIdx.x + RAD] = A[(y+RAD) * W + x];  \n","  }\n","\n","  // 3. copy the tile left-edge halo \n","  if (threadIdx.x < RAD) \n","    // edge\n","    if ((x-RAD) >= 0) \n","      A_s[threadIdx.y + RAD][threadIdx.x] = A[y * W + x - RAD];  \n","\n","  // 4. copy the tile right-edge halo \n","  if (threadIdx.x >= BmR) \n","    // edge\n","    if ((x+RAD) < W) \n","      A_s[threadIdx.y + RAD][threadIdx.x + 2*RAD] = A[y * W + x + RAD];  \n","      \n","\n","  // 5. copy the tile center <-> block\n","\tA_s[RAD + threadIdx.y][RAD + threadIdx.x] = A[y*W+x];\n","\t\n","  // END SHARED MEMORY LOADING\n","\n","\t__syncthreads();\n","\n","  \n","  if (blockIdx.x == 0 && blockIdx.y == 0 && threadIdx.x == 0 && threadIdx.y == 0) {\n","    printf(\"BLOCK(%d,%d) - TILE_WIDTH = %d\\n\",blockIdx.x, blockIdx.y, TILE_WIDTH);\n","    for (int i = 0; i < TILE_WIDTH; i++) {\n","      for (int j = 0; j < TILE_WIDTH; j++) \n","        printf(\"%1.0f \", A_s[i][j]);\n","\t\t  printf(\"\\n\");\n","    }\n","  }\n","\n","\tfloat conv_sum = 0.0;\n","\tfor (int i = 0; i < m; i++)\n","\t\tfor (int j = 0; j < m; j++)\n","\t\t\tconv_sum += A_s[threadIdx.y+i][threadIdx.x+j] * M_dev[i*m + j];\n","\t\n","  // store conv result\n","  B[x*W+y] = conv_sum;\n","}\n","\n","/*\n"," * Average filter\n"," */\n","void Avg_mask(float *mask) {\n","\tint n = MASK_SIZE;\n","\tfor (int i = 0; i < n*n; i++)\n","\t\tmask[i] = (float) 1.0 / (n * n);\n","}\n","\n","\n","/*\n"," * main\n"," */\n","int main(void) {\n","\n","  // check params\n","  if (MASK_RADIUS >= BLOCK_SIZE) {\n","    printf(\"ERROR: it holds only if MASK_RADIUS < BLOCK_SIZE!\\n\");\n","    return 1;\n","  }\n","\n","\tint nW = DATA_WIDTH;\n","  int nH = DATA_HEIGHT;\n","\tint b = BLOCK_SIZE;\n","\n","\tfloat M[MASK_SIZE*MASK_SIZE]; // const size\n","\tfloat *A, *B, *A_dev, *B_dev;\n","\tint datasize = nW * nH * sizeof(float);\n","  int masksize = MASK_SIZE*MASK_SIZE * sizeof(float);\n","\n","  printf(\"Data size: %.2f (MB)\\n\", (float)datasize/(1024.0*1024.0));\n","\tprintf(\"Initializing data...\\n\");\n","\tA = (float *) malloc(datasize);\n","\tB = (float *) malloc(datasize);\n","\n","\t// initialize data\n","\tfor (int i = 0; i < nH; i++)\n","\t\tfor (int j = 0; j < nW; j++)\n","\t\t\tA[i*nW+j] = rand()%10;\n","\n","  // initialize mask \n","\tAvg_mask(M);\n","\n","#if DEBUG\n","\t// print data\n","\tprintf(\"Print matrix A...\\n\");\n","\tfor (int i = 0; i < nH; i++) {\n","    if (i%8 == 0 && i>0)\n","      printf(\"\\n\");\n","\n","\t\tfor (int j = 0; j < nW; j++)\n","      if (j%8 == 0 && j>0)\n","\t\t\t  printf(\" %0.0f \", A[i*nW+j]);\n","      else\n","        printf(\"%0.0f \", A[i*nW+j]);\n","\t\tprintf(\"\\n\");\n","\t}\n","\n","\tprintf(\"Print matrix M ...\\n\");\n","\tfor (int i = 0; i < MASK_SIZE; i++) {\n","\t\tfor (int j = 0; j < MASK_SIZE; j++)\n","\t\t\t  printf(\" %1.2f \", M[i * MASK_SIZE + j]);\n","\t\tprintf(\"\\n\");\n","\t}\n","#endif\n","\n","\t// cuda allocation \n","\tCHECK(cudaMemcpyToSymbol(M_dev, M, masksize));\n","\tCHECK(cudaMalloc((void **) &A_dev, datasize));\n","\tCHECK(cudaMalloc((void **) &B_dev, datasize));\n","\tCHECK(cudaMemcpy(A_dev, A, datasize, cudaMemcpyHostToDevice));\n","\t\n","\t// block, grid dims, kernel\n","\tdim3 block(b, b);\n","\tdim3 grid((nW+b-1)/b, (nH+b-1)/b);\n","  double iStart, iElaps;\n","\tiStart = seconds();\n","\tconv2D<<<grid, block>>>(A_dev, B_dev);\n","  cudaDeviceSynchronize();\n","  iElaps = seconds() - iStart;\n","\tprintf(\"\\nconv2D<<<(%d,%d), (%d,%d)>>> elapsed time %f sec \\n\\n\", grid.x, grid.y, block.x, block.y, iElaps);\n","\tCHECK(cudaGetLastError());\n","\n","\tCHECK(cudaMemcpy(B, B_dev, datasize, cudaMemcpyDeviceToHost));\n","\n","#if DEBUG\n","\t// print out data\n","\tprintf(\"Print results...\\n\");\n","\tfor (int i = 0; i < nH; i++) {\n","    if (i%8 == 0 && i>0)\n","      printf(\"\\n\");\n","\t\tfor (int j = 0; j < nW; j++)\n","      if (j%8 == 0 && j>0)\n","\t\t\t  printf(\" %0.2f \", B[i*nW+j]);\n","      else\n","        printf(\"%0.2f \", B[i*nW+j]);\n","\t\tprintf(\"\\n\");\n","\t}\n","#endif\n","\n","\tcudaFree(A_dev);\n","\tcudaFree(B_dev);\n","  cudaDeviceReset();\n","\tfree(A);\n","\tfree(B);\n","\treturn 0;\n","}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yiun00TE2wcE"},"source":["# Compilazione ed esecuzione\n","!nvcc -arch=sm_60  conv2D.cu -o conv2D\n","!./conv2D"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TQ9BuU3ZW_zL"},"source":["!nvprof conv2D"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SscxM5gEXJy2"},"source":[""],"execution_count":null,"outputs":[]}]}